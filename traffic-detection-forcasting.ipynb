{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12759716,"sourceType":"datasetVersion","datasetId":8066044},{"sourceId":12796803,"sourceType":"datasetVersion","datasetId":8090728}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:30:52.257946Z","iopub.execute_input":"2025-08-25T16:30:52.258214Z","iopub.status.idle":"2025-08-25T16:30:55.086423Z","shell.execute_reply.started":"2025-08-25T16:30:52.258191Z","shell.execute_reply":"2025-08-25T16:30:55.085230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 0: Setup & Config ===\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\n\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\n\n# ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n\n# ---- Configure your paths here ----\nRM_PATH_GLOB = \"/kaggle/input/traffic-dats/omlstreaming-mrc-traffic-detection-dataset-97f954e/rm/*.csv\"   # e.g., \"/data/huawei/RM/*.csv\"\n\n# Core parameters\nTIME_WINDOW = \"5min\"    # aggregation window\nLAG_STEPS = [1, 2, 3, 6, 12]  # with 5-min windows -> up to 60 min history\nROLL_WINDOWS = [3, 6, 12]     # rolling statistics windows (in 5-min steps)\nFORECAST_HORIZONS = [1, 3]    # predict t+1 (5 min ahead), t+3 (15 min ahead)\n\nRANDOM_STATE = 42\n\n@dataclass\nclass SplitBoundaries:\n    train_end: pd.Timestamp\n    val_end: pd.Timestamp\n    # test is everything > val_end\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:30:55.087950Z","iopub.execute_input":"2025-08-25T16:30:55.088480Z","iopub.status.idle":"2025-08-25T16:30:57.505543Z","shell.execute_reply.started":"2025-08-25T16:30:55.088444Z","shell.execute_reply":"2025-08-25T16:30:57.504490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport glob\nimport os\n\n# Path to RM folder\nfolder_path = \"/kaggle/input/traffic-data/omlstreaming-mrc-traffic-detection-dataset-97f954e/rm\"\n# Column names from dataset description\ncolumns = [\n    \"anonymized_car_plate\",\n    \"startDate\",\n    \"startTime\",\n    \"endDate\",\n    \"endTime\",\n    \"leaving_edge\",\n    \"intersection\",\n    \"incoming_edge\"\n]\n\n# Load and combine\nall_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\ndf_list = []\nfor file in all_files:\n    df = pd.read_csv(file, names=columns, header=None)\n    df_list.append(df)\n\ndata = pd.concat(df_list, ignore_index=True)\n\n# Combine start date and time into a datetime\ndata[\"start_datetime\"] = pd.to_datetime(data[\"startDate\"] + \" \" + data[\"startTime\"])\ndata[\"end_datetime\"] = pd.to_datetime(data[\"endDate\"] + \" \" + data[\"endTime\"])\n\nprint(\"Data shape:\", data.shape)\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:30:57.506671Z","iopub.execute_input":"2025-08-25T16:30:57.507152Z","iopub.status.idle":"2025-08-25T16:31:13.168703Z","shell.execute_reply.started":"2025-08-25T16:30:57.507118Z","shell.execute_reply":"2025-08-25T16:31:13.167627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:13.169701Z","iopub.execute_input":"2025-08-25T16:31:13.169982Z","iopub.status.idle":"2025-08-25T16:31:13.199515Z","shell.execute_reply.started":"2025-08-25T16:31:13.169958Z","shell.execute_reply":"2025-08-25T16:31:13.198251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ncat_cols = ['anonymized_car_plate', 'leaving_edge', 'intersection', 'incoming_edge']\n\nlabel_encoders = {}\nfor col in cat_cols:\n    le = LabelEncoder()\n    data[col] = le.fit_transform(data[col].astype(str))\n    label_encoders[col] = le\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:13.202217Z","iopub.execute_input":"2025-08-25T16:31:13.202584Z","iopub.status.idle":"2025-08-25T16:31:20.758321Z","shell.execute_reply.started":"2025-08-25T16:31:13.202552Z","shell.execute_reply":"2025-08-25T16:31:20.757369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\ndata.set_index('start_datetime').resample('H').size().plot(figsize=(12,6))\nplt.title(\"Traffic Counts Per Hour\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:20.762905Z","iopub.execute_input":"2025-08-25T16:31:20.763272Z","iopub.status.idle":"2025-08-25T16:31:24.588150Z","shell.execute_reply.started":"2025-08-25T16:31:20.763239Z","shell.execute_reply":"2025-08-25T16:31:24.586986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['duration_seconds'] = (data['end_datetime'] - data['start_datetime']).dt.total_seconds()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:24.589317Z","iopub.execute_input":"2025-08-25T16:31:24.589836Z","iopub.status.idle":"2025-08-25T16:31:24.668478Z","shell.execute_reply.started":"2025-08-25T16:31:24.589792Z","shell.execute_reply":"2025-08-25T16:31:24.667339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\ndata['intersection'].value_counts().head(10).plot(kind='bar')\nplt.title(\"Top Intersections\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:24.669361Z","iopub.execute_input":"2025-08-25T16:31:24.669698Z","iopub.status.idle":"2025-08-25T16:31:24.884086Z","shell.execute_reply.started":"2025-08-25T16:31:24.669667Z","shell.execute_reply":"2025-08-25T16:31:24.883177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Plot histogram\n# plt.figure(figsize=(10,6))\n# plt.hist(vehicle_counts['vehicle_count'], bins=30, edgecolor='black', alpha=0.7)\n# plt.axvline(low_th, color='green', linestyle='--', linewidth=2, label=f'Low Threshold ({low_th:.1f})')\n# plt.axvline(med_th, color='orange', linestyle='--', linewidth=2, label=f'Medium Threshold ({med_th:.1f})')\n\n# plt.title('Vehicle Count Distribution with Thresholds')\n# plt.xlabel('Vehicle Count')\n# plt.ylabel('Frequency')\n# plt.legend()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:24.885137Z","iopub.execute_input":"2025-08-25T16:31:24.885421Z","iopub.status.idle":"2025-08-25T16:31:24.890684Z","shell.execute_reply.started":"2025-08-25T16:31:24.885400Z","shell.execute_reply":"2025-08-25T16:31:24.889684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nBasic statistics:\")\nprint(data.describe())\n\n# 7. Plot trip duration distribution\nplt.figure(figsize=(8, 4))\nsns.histplot(data['duration_seconds'], bins=50, kde=True)\nplt.title(\"Distribution of Trip Duration (seconds)\")\nplt.show()\n\n# 8. Count of unique cars\nprint(\"\\nUnique cars:\", data['anonymized_car_plate'].nunique())\n\n# 9. Trips per intersection\nplt.figure(figsize=(10, 5))\nsns.countplot(y='intersection', data=data, order=data['intersection'].value_counts().index)\nplt.title(\"Trips per Intersection\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:24.891726Z","iopub.execute_input":"2025-08-25T16:31:24.892014Z","iopub.status.idle":"2025-08-25T16:31:39.129995Z","shell.execute_reply.started":"2025-08-25T16:31:24.891990Z","shell.execute_reply":"2025-08-25T16:31:39.128947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n#  Combine date and time into a single datetime column\ndata['start_datetime'] = pd.to_datetime(data['startDate'] + ' ' + data['startTime'])\n\n#  Define time window (e.g., hourly)\ndata['time_window'] = data['start_datetime'].dt.floor('H')  # Hourly aggregation\n\n# Count vehicles per intersection per time window\nvehicle_counts = (\n    data.groupby(['intersection', 'time_window'])\n    .size()\n    .reset_index(name='vehicle_count')\n)\n\n# Define congestion thresholds\n\nlow_th = np.percentile(vehicle_counts['vehicle_count'], 33)\nmed_th = np.percentile(vehicle_counts['vehicle_count'], 66)\n\ndef classify_congestion(count):\n    if count <= low_th:\n        return \"Low\"\n    elif count <= med_th:\n        return \"Medium\"\n    else:\n        return \"High\"\n\n\nvehicle_counts['congestion_level'] = vehicle_counts['vehicle_count'].apply(classify_congestion)\n\n#  Merge back to original data if needed\ndata = pd.merge(data, vehicle_counts, on=['intersection', 'time_window'], how='left')\n\n#  Encode congestion labels for ML\nlabel_map = {\"Low\": 0, \"Medium\": 1, \"High\": 2}\ndata['congestion_label'] = data['congestion_level'].map(label_map)\n\nprint(data[['intersection', 'time_window', 'vehicle_count', 'congestion_level', 'congestion_label']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:39.131391Z","iopub.execute_input":"2025-08-25T16:31:39.131746Z","iopub.status.idle":"2025-08-25T16:31:41.722614Z","shell.execute_reply.started":"2025-08-25T16:31:39.131715Z","shell.execute_reply":"2025-08-25T16:31:41.721244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n#  Feature engineering\ndata['day_of_week'] = data['time_window'].dt.dayofweek  # Monday=0\ndata['hour_of_day'] = data['time_window'].dt.hour       # 0-23\n\n# Encode intersection\nle_intersection = LabelEncoder()\ndata['intersection_encoded'] = le_intersection.fit_transform(data['intersection'])\n\n#  Select features and target\nX = data[['intersection_encoded', 'day_of_week', 'hour_of_day']]\ny = data['congestion_label']\n\n#  Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predictions\ny_pred = rf.predict(X_test)\n\n# Evaluation\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:31:41.723740Z","iopub.execute_input":"2025-08-25T16:31:41.724031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Initialize XGBoost model\nxgb_model = XGBClassifier(\n    n_estimators=200,       # number of trees\n    max_depth=6,           # depth of each tree\n    learning_rate=0.1,     # step size shrinkage\n    subsample=0.8,         # use 80% of data for each tree\n    colsample_bytree=0.8,  # use 80% of features per tree\n    random_state=42,\n    eval_metric='mlogloss' # for multi-class classification\n)\n\n# Train the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Ensure we have the hourly (or whatever) window and counts\n# If you've already created vehicle_counts earlier and merged back, we’ll (re)aggregate to be safe.\nts = (\n    data\n    .groupby(['intersection','time_window'])\n    .size()\n    .rename('vehicle_count')\n    .reset_index()\n    .sort_values(['intersection','time_window'])\n)\n\nprint(ts.head(), ts.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts['vehicle_count'].mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LAGS = [1, 2, 3, 6, 12]        # with hourly windows → up to 12 hours history\nROLLS = [3, 6, 12]             # rolling stats windows\nHORIZONS = [1, 3]              # predict t+1 and t+3 steps ahead\n\ndef add_features(df):\n    def _fe(g):\n        g = g.sort_values('time_window').copy()\n        # lags\n        for L in LAGS:\n            g[f'lag_{L}'] = g['vehicle_count'].shift(L)\n        # rolling stats (shifted to avoid leakage)\n        for W in ROLLS:\n            g[f'roll_mean_{W}'] = g['vehicle_count'].shift(1).rolling(W, min_periods=1).mean()\n            g[f'roll_std_{W}']  = g['vehicle_count'].shift(1).rolling(W, min_periods=1).std()\n            g[f'roll_max_{W}']  = g['vehicle_count'].shift(1).rolling(W, min_periods=1).max()\n        # calendar features\n        g['hour'] = g['time_window'].dt.hour\n        g['dow']  = g['time_window'].dt.dayofweek\n        g['hour_sin'] = np.sin(2*np.pi*g['hour']/24.0)\n        g['hour_cos'] = np.cos(2*np.pi*g['hour']/24.0)\n        g['dow_sin']  = np.sin(2*np.pi*g['dow']/7.0)\n        g['dow_cos']  = np.cos(2*np.pi*g['dow']/7.0)\n        # future targets\n        for h in HORIZONS:\n            g[f'y_tplus_{h}'] = g['vehicle_count'].shift(-h)\n        return g\n    out = df.groupby('intersection', group_keys=False).apply(_fe)\n    return out\n\nfeat = add_features(ts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop rows with any NA in features or targets\nfeature_cols = (\n    [f'lag_{L}' for L in LAGS] +\n    sum([[f'roll_mean_{w}', f'roll_std_{w}', f'roll_max_{w}'] for w in ROLLS], []) +\n    ['hour_sin','hour_cos','dow_sin','dow_cos']\n)\n\ntarget_cols = [f'y_tplus_{h}' for h in HORIZONS]\n\nclean = feat.dropna(subset=feature_cols + target_cols).copy()\n\n# Temporal boundaries: 70% train, 15% val, 15% test by global time\nall_times = np.sort(clean['time_window'].unique())\nt_train_end = all_times[int(0.7*len(all_times))-1]\nt_val_end   = all_times[int(0.85*len(all_times))-1]\n\ntrain_mask = clean['time_window'] <= t_train_end\nval_mask   = (clean['time_window'] > t_train_end) & (clean['time_window'] <= t_val_end)\ntest_mask  = clean['time_window'] > t_val_end\n\nprint(\"Train end:\", t_train_end, \"| Val end:\", t_val_end)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ndef rmse(y_true, y_pred): \n    return mean_squared_error(y_true, y_pred, squared=False)\n\nmodels = {}\nfor h in HORIZONS:\n    y_tr = clean.loc[train_mask, f'y_tplus_{h}']\n    y_va = clean.loc[val_mask,   f'y_tplus_{h}']\n    y_te = clean.loc[test_mask,  f'y_tplus_{h}']\n\n    X_tr = clean.loc[train_mask, feature_cols]\n    X_va = clean.loc[val_mask,   feature_cols]\n    X_te = clean.loc[test_mask,  feature_cols]\n\n    reg = RandomForestRegressor(\n        n_estimators=400,\n        max_depth=None,\n        n_jobs=-1,\n        random_state=42\n    )\n    reg.fit(X_tr, y_tr)\n    models[h] = reg\n\n    # eval\n    pred_va = reg.predict(X_va)\n    pred_te = reg.predict(X_te)\n\n    print(f\"\\n=== Horizon t+{h} ===\")\n    print(\"VAL: RMSE:\", rmse(y_va, pred_va), \" MAE:\", mean_absolute_error(y_va, pred_va))\n    print(\"TEST: RMSE:\", rmse(y_te, pred_te), \" MAE:\", mean_absolute_error(y_te, pred_te))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# thresholds from TRAIN vehicle_count distribution\ntrain_counts = clean.loc[train_mask, 'vehicle_count']\nq33 = np.percentile(train_counts, 33)\nq66 = np.percentile(train_counts, 66)\n\ndef flow_to_class(x, q33=q33, q66=q66):\n    if x <= q33: return 0\n    elif x <= q66: return 1\n    else: return 2\n\nfor h, reg in models.items():\n    X_te = clean.loc[test_mask, feature_cols]\n    y_true_flow_future = clean.loc[test_mask, f'y_tplus_{h}']\n    y_pred_flow_future = reg.predict(X_te)\n\n    y_true_cls = [flow_to_class(v) for v in y_true_flow_future]\n    y_pred_cls = [flow_to_class(v) for v in y_pred_flow_future]\n\n    from sklearn.metrics import accuracy_score, f1_score\n    print(f\"\\nFuture congestion via forecast — t+{h}\")\n    print(\" Acc:\", accuracy_score(y_true_cls, y_pred_cls),\n          \" F1(macro):\", f1_score(y_true_cls, y_pred_cls, average='macro'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assume df['traffic_flow'] is your series\nscaler = MinMaxScaler()\ntraffic_scaled = scaler.fit_transform(data[['vehicle_count']])\n\n# function to create sequences\ndef create_sequences(data, past_steps=24, future_steps=3):\n    X, y = [], []\n    for i in range(len(data) - past_steps - future_steps + 1):\n        X.append(data[i:i+past_steps])\n        y.append(data[i+past_steps:i+past_steps+future_steps])\n    return np.array(X), np.array(y)\n\n# prepare data\nPAST_STEPS = 24  # using last 24 time steps\nFUTURE_STEPS = 3 # predicting t+1, t+2, t+3\nX, y = create_sequences(traffic_scaled, PAST_STEPS, FUTURE_STEPS)\n\n# train/test split\ntrain_size = int(len(X) * 0.8)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# reshape for LSTM [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_test  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# build LSTM model\nmodel = Sequential([\n    LSTM(64, activation='relu', input_shape=(PAST_STEPS, 1)),\n    Dense(FUTURE_STEPS)  # output is multi-step forecast\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict(X_test)\n\n# invert scaling\ny_pred_inv = scaler.inverse_transform(\n    np.concatenate((y_pred, np.zeros((y_pred.shape[0], 1))), axis=1)\n)[:, :FUTURE_STEPS]\n\ny_test_2d = y_test.reshape(y_test.shape[0], y_test.shape[1] * y_test.shape[2])  # (578830, 3)\n\n# Concatenate zeros for the missing columns scaler expects\ny_test_inv = scaler.inverse_transform(\n    np.concatenate((y_test_2d, np.zeros((y_test_2d.shape[0], 1))), axis=1)\n)[:, :FUTURE_STEPS]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\nfor step in range(FUTURE_STEPS):\n    mae = mean_absolute_error(y_test_inv[:, step], y_pred_inv[:, step])\n    rmse = mean_squared_error(y_test_inv[:, step], y_pred_inv[:, step], squared=False)\n    print(f\"Horizon t+{step+1}: MAE={mae:.2f}, RMSE={rmse:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}